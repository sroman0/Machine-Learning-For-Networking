{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30d177e-5b4b-40e2-9b01-643db6401362",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Machine Learning for Networking <b><center>\n",
    "<center><b><font size=6>Lab 9 <b><center>\n",
    "<center><b><font size=6>    Unsupervised learning: Clustering<b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17982c4f-27b6-434a-b082-f0435778a095",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Objective: Learn how to use different clustering techniques\n",
    "\n",
    "Clustering is unsupervised learning. We want to assign data samples to different groups based on their similarity/proximity. As a result, we can derive, explain, and utilize the properties of such groups. We do not have nor use labels for clustering task, and we use unsupervised metrics to evaluate the performance. However, we can also use labeled data to better understand the obtained clusters with supervised metrics. \n",
    "\n",
    "1. **k-Means** is a hard clustering approach (each sample is associated with one and only one cluster), that partitions m observations to k clusters in which each observation belongs to the cluster with the nearest mean (cluster centroid). Useful links: <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">Wiki</a>, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\">K-means in sklearn</a>.\n",
    "2. **Gaussian mixture model (GMM)** is a soft clustering appproach that provides the degree of beloging of each of the m observation to k different clusters. Each cluster is represented by a gaussian distribution.  Useful links: <a href=\"https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model\">Wiki</a>, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html\">GMM in sklearn</a>.\n",
    "3. **Density-based spatial clustering of applications with noise (DBSCAN)** is a density-based hard clustering algorithm, that groups together samples that are connected together in an empirical graph, defining core points, border points, and outliers. Useful links: <a href=\"https://en.wikipedia.org/wiki/DBSCAN\">Wiki</a>, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN\">DBSCAN in sklearn</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4126eda-4db8-4349-97db-2c8deb1378e9",
   "metadata": {},
   "source": [
    "### 1. Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a825d89-9c5b-4d55-a4d7-bd2501e009f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To upgrade scikit learn to latest version. Then restart kernel. You need to run this cell only once.\n",
    "!python -m pip install scikit-learn --upgrade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f7ac0-b9fa-4b14-8dc4-2b838c2bf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for the tutorial\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610b4bb-a9bd-4540-af53-ec00194fa042",
   "metadata": {},
   "source": [
    "## 2. Tutorial - Clustering\n",
    "\n",
    "In the tutorial we will use the classical IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feae026-21d8-4358-98c6-61e36532d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset of IRIS flowers\n",
    "from sklearn import datasets\n",
    "iris_data = datasets.load_iris()\n",
    "features_iris = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "df_iris = pd.DataFrame(iris_data.data, columns = features_iris)\n",
    "df_iris['type'] = 'setosa'\n",
    "df_iris.loc[50:99, 'type'] = 'versicolor'\n",
    "df_iris.loc[100:149, 'type'] = 'virginica'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae244d0-58cf-45bc-977f-d2f47e0da557",
   "metadata": {},
   "source": [
    "## 2.1 k-Means\n",
    "\n",
    "k-Means needs as hyper-parameter the number of clusters k.\n",
    "Another hyper-parameter is the initialization strategy for the centroid ``init``:\n",
    "- *random*: centroids will be initialized randomly in the valid data region. You can specify the ``random_state``. Changing the random state may change the result due to a different centroid inizialization.\n",
    "- *k-means++*: centroids will be equally distant from each other, leading to provably better results than random initialization.\n",
    "- ndarray: if you have a clue about where centroids could be, you can pass them (i.e. their coordinates) as initialization.\n",
    "\n",
    "Moreover, ``n_init`` is the number of times the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of clustering error. \n",
    "\n",
    "For the tutorial, we will not perform any pre-processing step and directly apply k-Means.\n",
    "To create the clusters the ``.fit()`` function creates the clustering function and clusters, by minimizing the clustering error on the data.\n",
    "\n",
    "Here we are using n_init=1 and random initialization. Try to change the used features ``features_iris``, and the hyper-parameters ``n_clusters`` and ``random_state`` and see what changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865b406-1e63-40d5-8e28-d4b786eeaa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_iris = ['sepal_length', 'petal_length'] # the four features are ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, init='random', random_state=None, n_init=1)\n",
    "kmeans.fit(df_iris[features_iris])\n",
    "\n",
    "# print the clustered labels\n",
    "print('The clustered labels are:\\n', kmeans.labels_)\n",
    "print()\n",
    "\n",
    "# print the centroid of each feature for each cluster\n",
    "    print('The centroids are:\\n', kmeans.cluster_centers_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a60e1f-714e-44d9-8aac-a44735e76356",
   "metadata": {},
   "source": [
    "You can also visualize the points and their assignments. The following function will show the result if you have 2 features (2D scatter plot). If you have more dimensions, you can plot all the couples of features, or first perform dimensionality reduction (e.g., PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91f98b-bd27-458f-90c0-0b5f3fe2084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot the clustering -- only 2 dimensione (if more, it will plot only the first two columns).\n",
    "def plot_clustering(X, labels, centers=None, title=\"\", subplot=None):\n",
    "\n",
    "    # Plot in given subplot\n",
    "    if subplot:\n",
    "        plt.subplot(subplot)\n",
    "\n",
    "    # Plot data with labels as color\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=labels)\n",
    "\n",
    "    # Plot centers if given\n",
    "    if centers is not None:\n",
    "        plt.scatter(\n",
    "            centers[:, 0], centers[:, 1], c=\"red\", s=150, alpha=0.9, label=\"Centers\"\n",
    "        )\n",
    "\n",
    "    # Set title\n",
    "    plt.title(title)\n",
    "    plt.xlabel(X.columns[0])\n",
    "    plt.ylabel(X.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b3359-86ab-4f7e-9063-ec9f22e9bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering(df_iris[features_iris], kmeans.labels_, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b393a4-aa8d-4258-b2e6-ae343b70e6e4",
   "metadata": {},
   "source": [
    "### Unsupervised clustering performance\n",
    "\n",
    "After having computed the clusters, we can obtain clustering performance metrics:\n",
    "\n",
    "- the metric minimized in the clustering algorithm for the empirical data (for k-Means, the mean squared distance of the points from their own centroid)\n",
    "- the silhouette to measure the similarity of a sample to its own cluster, compared to other clusters (<a href=\"https://en.wikipedia.org/wiki/Silhouette_(clustering)\">Wiki</a>)\n",
    "\n",
    "See how the results change by changing the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e2c15-c9e2-4349-86f5-e2b3dad19a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum of squared distances of samples to their closest cluster center.\n",
    "print(\"Sum of squared distances of the samples from their centroid: \", kmeans.inertia_)\n",
    "# If we divide it by number of sample m, it is the clustering error we have seen in theoretical lectures\n",
    "print(\"Mean squared distances of the samples from their centroid: \", kmeans.inertia_/len(df_iris))\n",
    "\n",
    "# To compute the silhouette, sklearn provides the silhouette_score in the metrics sklearn.metrics library\n",
    "print(\"Silhouette score of the samples: \",silhouette_score(df_iris[features_iris], kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a85f78-4df6-4466-ba44-889abb722f95",
   "metadata": {},
   "source": [
    "We can also compare clusters with the ground-truth labels (or with a different clustering):\n",
    "- Rand index (RI): given the knowledge of the ground truth class assignments labels_true and our clustering algorithm assignments of the same samples labels_pred, the Rand index is a function that measures the similarity of the two assignments,\n",
    "- Adjusted rand index (ARI): corrects the RI for chance and will give a baseline at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d205191-772f-40ef-9138-3929fcdbbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the rand index and adjusted rand index sklearn provides the rand_score and adjusted_rand_score functions\n",
    "rand_score_result = rand_score(df_iris[\"type\"], kmeans.labels_)\n",
    "adj_rand_score_result = adjusted_rand_score(df_iris[\"type\"], kmeans.labels_)\n",
    "\n",
    "print(\"RI between clustering and given classes is \",rand_score_result)\n",
    "print(\"ARI between clustering and given classes is \",adj_rand_score_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b941959-001d-45b0-9d34-de537460acb8",
   "metadata": {},
   "source": [
    "### Performance by changing k\n",
    "\n",
    "k can be chosen based on different criteria. For example:\n",
    "- Defined by application \n",
    "- Desired compression rate\n",
    "- Elbow (or knee) method \n",
    "- Validation error\n",
    "\n",
    "Here we will run the clustering algorithms with different values of k and evaluate the performances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856d438-9d99-44b2-bf3c-150b317ba2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we only have to run k-means with different values of k. \n",
    "# You could also run k-means with the same value of k but different random_seed to check different centroids initialization.\n",
    "\n",
    "#choose the features you want to work on  \n",
    "features_iris = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'] # the five features are ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "kmeans_results = []\n",
    "for k in range(2, 21):\n",
    "    kmeans_results.append(KMeans(n_clusters=k).fit(df_iris[features_iris]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc613d9-0183-463e-bd6b-17f09d270b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(8, 3.5))\n",
    "\n",
    "axs[0].plot(\n",
    "    [k for k in range(2,21)], \n",
    "    [result.inertia_ for result in kmeans_results]\n",
    ")\n",
    "axs[0].set_xlabel(\"Number of clusters k\")\n",
    "axs[0].set_ylabel(\"Clustering error\")\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(\n",
    "    [k for k in range(2,21)], \n",
    "    [silhouette_score(df_iris[features_iris], result.labels_) for result in kmeans_results]\n",
    ")\n",
    "axs[1].set_xlabel(\"Number of clusters k\")\n",
    "axs[1].set_ylabel(\"Silhouette\")\n",
    "axs[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11293b61-1108-404e-a3da-c8b298eadede",
   "metadata": {},
   "source": [
    "Here the two plots show contrasting objectives: for clustering error, increasing k will improve the results, while for silhouette increasing k will worsen the results. For clustering error, the addition of an additional cluster cannot result in a larger average distance from the points to their corresponding cluster means. It naturally follows that the raw clustering error will always favour a larger number. Hence, by looking at the silhouette a small number of k is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef0ef6-a88d-49c1-b8f8-6b4b47f6bf18",
   "metadata": {},
   "source": [
    "## 2.2 Gaussian Mixture Model\n",
    "\n",
    "GMM need as hyper-parameter the number of clusters k (``n_components``).\n",
    "\n",
    "Another hyper-parameter is the initialization strategy for the centroids ``init_params``:\n",
    "- ‘kmeans’ : responsibilities are initialized using k-Means.\n",
    "- ‘random’ : responsibilities are initialized randomly.\n",
    "\n",
    "For the tutorial, we will not perform any pre-processing step and directly apply GMM.\n",
    "To create the clusters the ``.fit()`` function creates the clustering function and clusters, by maximizing the log-likelihood on the data.\n",
    "\n",
    "Try to change the used features ``features_iris``, and the hyper-parameters  and see what changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41a916-f8ba-4724-b5b3-983ddbc6887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_iris = ['sepal_length', 'petal_length'] # the five features are ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, init_params='kmeans')\n",
    "gmm.fit(df_iris[features_iris])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71846c3-1e99-4369-94c0-f5a54171af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the sample mean vector of each gaussian\n",
    "print('The centroids are:\\n', gmm.means_)\n",
    "print()\n",
    "\n",
    "# print the sample covariance matrix of each gaussian\n",
    "print('The covariance matrices are:\\n', gmm.covariances_)\n",
    "print()\n",
    "\n",
    "# print the effective size of each cluster\n",
    "print('The effective sizes are:\\n', gmm.weights_)\n",
    "#can be also computed as  [gmm_probs[:,0].sum()/len(gmm_probs), gmm_probs[:,1].sum()/len(gmm_probs),gmm_probs[:,2].sum()/len(gmm_probs) ]\n",
    "print()\n",
    "\n",
    "# print the degree of belonging to each cluster\n",
    "gmm_probs = gmm.predict_proba(df_iris[features_iris])\n",
    "with np.printoptions(precision=4, suppress=True): #to print only the first 4 digits\n",
    "    print('The degrees of belonging (probabilities) for the samples are:\\n', gmm_probs)\n",
    "print()\n",
    "      \n",
    "# print the clustered labels\n",
    "gmm_labels = gmm.predict(df_iris[features_iris])\n",
    "print('The clustered labels are:\\n', gmm_labels)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8d7bd-2c26-4b05-8b98-8538b259523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the cluster (only first two dimensions)\n",
    "plot_clustering(df_iris[features_iris], gmm_labels, gmm.means_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1db46d-580b-479f-9bf3-0b070ebefc92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Unsupervised clustering performance\n",
    "\n",
    "After having computed the clusters, measure the log-likelihood and the average silhouette. \n",
    "See how the results change by changing the hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcae6c8-6d73-4e5d-ab83-b03fea45adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum of logarithm of probability to sample data points under Gaussian mixture model (higher is better)\n",
    "print(\"Total log-likelihood score: \", gmm.score(df_iris[features_iris]) )\n",
    "\n",
    "# To compute the silhouette, sklearn provides the silhouette_score in the metrics sklearn.metrics library\n",
    "print(\"Silhouette score of the samples: \",silhouette_score(df_iris[features_iris], gmm_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b23055e-60c2-4591-a58c-15b7d3d2f108",
   "metadata": {},
   "source": [
    "As previously, compare clusters with the ground-truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c7c0e-6a3c-4821-b83c-7aa7e9f46744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the rand index and adjusted rand index sklearn provides the rand_score and adjusted_rand_score functions\n",
    "\n",
    "rand_score_result = rand_score(df_iris[\"type\"], gmm_labels)\n",
    "adj_rand_score_result = adjusted_rand_score(df_iris[\"type\"], gmm_labels)\n",
    "\n",
    "print(\"RI between clustering and given classes is \",rand_score_result)\n",
    "print(\"ARI between clustering and given classes is \",adj_rand_score_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f1a48-ba17-4ea7-8788-96908557d872",
   "metadata": {},
   "source": [
    "## 2.3 Density Based Clustering DBSCAN\n",
    "\n",
    "Both k-means and GMM cluster data points using a distance measure from a reference point, which is a natural measure of similarity in many cases. However, in some applications, the data conforms to a different (non-Euclidian) structure. In this case, there are clustering techniques based on the notion of connectivity. Here, two data points are considered similar if they can be reached by intermediate data points that have a small (Euclidean) distance. \n",
    "\n",
    "In contrast to k-means and the GMM, DBSCAN does not require the number of clusters to be pre-defined -  the obtained number will depend on its parameters. Moreover, DBSCAN detects outliers that are interpreted as degenerated clusters consisting of a single data point.\n",
    "\n",
    "DBSCAN requires specifying two parameters  `eps` and `min_samples`. The meaning of these parameters are well explained [here](https://scikit-learn.org/stable/modules/clustering.html#dbscan).  We need to set the eps and min_samples parameters, and tune DBSCAN epsilon based on Silhouette index.\n",
    "\n",
    "For the tutorial, we will not perform any pre-processing step and directly apply DBSCAN.\n",
    "To create the clusters the ``.fit()`` function creates the clustering function and clusters.\n",
    "\n",
    "Try to change the used features ``features_iris``, and the hyper-parameters  and see what changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9dd7dd-7038-4c26-9dd5-7e041a097cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_iris = ['sepal_length', 'petal_length'] # the five features are ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "# fit the DBSCAN model\n",
    "dbscan = DBSCAN(eps=0.4, min_samples=3)\n",
    "dbscan.fit(df_iris[features_iris])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39abe7-00a4-4635-a24b-065f20009b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the labels generated by DBSCAN might contain -1 values as result, representing outliers\n",
    "# print the clustered labels\n",
    "print('The clustered labels are:\\n', dbscan.labels_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d6974-7e82-4706-93f6-61613d74aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)\n",
    "n_noise_ = list(dbscan.labels_).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefa637-4373-47cf-aa01-067d9b6b24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the clusters (only first two dimensions)\n",
    "plot_clustering(df_iris[[\"sepal_length\", \"petal_length\"]], dbscan.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422f08e-7977-4c71-9b89-6f1326715f25",
   "metadata": {},
   "source": [
    "Tune DBSCAN based on silhouette score. Here we only tune eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d36e8-98b1-449d-8a19-bcabc2b7a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_results = []\n",
    "for eps in np.arange(0.05, 0.9, .1): #from 0.05 to 0.9 with steps of 0.1\n",
    "    dbscan_tmp = DBSCAN(eps=eps, min_samples=3)\n",
    "    dbscan_tmp.fit(df_iris[features_iris])\n",
    "    dbscan_results.append((eps, dbscan_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba937d-0619-462d-8acd-bd0e9eaabd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_tmp = DBSCAN(eps=0.05, min_samples=3)\n",
    "dbscan_tmp.fit(df_iris[features_iris])\n",
    "dbscan_tmp.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374286ca-a2a7-4512-a02f-a2b76de9bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes_dbscan = []\n",
    "silhouettes_dbscan_noOutliers= []\n",
    "\n",
    "for eps, dbscan_result in dbscan_results:\n",
    "\n",
    "    # compute silhouette score\n",
    "    score = silhouette_score(df_iris[features_iris], dbscan_result.labels_)\n",
    "    silhouettes_dbscan.append((eps, score))\n",
    "    \n",
    "    # compute silhouette score only of clustered samples (not noise, i.e., not -1)\n",
    "    score = silhouette_score(\n",
    "        df_iris[~df_iris.index.isin(np.where(dbscan_result.labels_ == -1)[0])][features_iris], \n",
    "        np.delete(dbscan_result.labels_, np.where(dbscan_result.labels_ == -1))\n",
    "    )\n",
    "    silhouettes_dbscan_noOutliers.append((eps, score))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eba393-bb2b-4e20-bf9b-48e22599be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(8, 3.5))\n",
    "\n",
    "axs[0].plot(\n",
    "    [eps for (eps, _) in silhouettes_dbscan], \n",
    "    [score for (_, score) in silhouettes_dbscan] \n",
    ")\n",
    "axs[0].set_xlabel(\"eps\")\n",
    "axs[0].set_ylabel(\"Silhouette\")\n",
    "axs[0].set_title(\"Average silhouette for all points\")\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(\n",
    "    [eps for (eps, _) in silhouettes_dbscan_noOutliers], \n",
    "    [score for (_, score) in silhouettes_dbscan_noOutliers] \n",
    ")\n",
    "axs[1].set_xlabel(\"eps\")\n",
    "axs[1].set_ylabel(\"Silhouette\")\n",
    "axs[1].set_title(\"Average silhouette without considering outliers\")\n",
    "axs[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572960e-c0fb-4985-aeed-9562507d223d",
   "metadata": {},
   "source": [
    "Considering all points, silhouette becomes very low with a small epsilon. This is because all the samples will be considered noise/outliers. Best results are obtained for larger epsilon (with 2 clusters). Notice that if epsilon becomes very large (e.g., eps=3) all points will be in a single cluster, and silhouette cannot be computed. The second plot accounts only for points that are clustered (removing outliers). A very small eps here seems to suggest better results: however, most of the points are considered noise and only very few points are in the found clusters, with high silhouette (points close to each other and far from the other clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e80fd-4471-4bb2-9264-220d8f19072e",
   "metadata": {},
   "source": [
    "## 3. Exercise - Darknet traffic\n",
    "\n",
    "A darknet is a network of passive IP addresses not hosting any services. Thus, all the darknet ports are open and the received traffic is stored. In this laboratory, you will work on traces referred to darknet traffic. \n",
    "\n",
    "You will develop an unsupervised ML pipeline for clustering the source IP addresses that send traffic to the darknet.  Firstly, you are asked to generate features for the source IP addresses from the raw dataset per packet. Then you will explore three different clustering algorithms (k-Means, GMM and DBSCAN) and analyze the results. IPs belonging to the same cluster should perform similar activities. \n",
    "\n",
    "We also provide you with the labels of these IP addresses as Ground Truth (GT). Each label is referred to a project running Internet scanning, mapping or security services. Therefore, you can compare the obtained unsupervised clustering with the GT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecf904-5986-4c28-86e5-4057a7f74dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for the exercise\n",
    "import pandas\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3bee8f-0c30-49d6-ab64-a8fa4f523440",
   "metadata": {},
   "source": [
    "### 3.1 Features generation and pre-processing\n",
    "In the file ``darknet_traces.csv`` you will find logs of darknet traffic. Each record reports the following information:\n",
    "- ts: timestamp of the received packet\n",
    "- src_ip: source IP address from which the packet was sent\n",
    "- src_port: source port from which the packet was sent\n",
    "- dst_ip: darknet IP address that the packet reached\n",
    "- dst_port: darknet port the packet reached\n",
    "- proto: protocol used\n",
    "- pck_len: length of the packet (bytes)\n",
    "- ttl: packet time to live (seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1319a-2739-4f1f-88c6-42270a0c3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4fac3a-5b02-4a25-9c84-90387417a9e0",
   "metadata": {},
   "source": [
    "In this step you are asked to create a dataset with the features for each source IP address. \n",
    "Create the following 5 features (however, you can repeat the exercise with other features)\n",
    "\n",
    "- Number of packets sent\n",
    "- Average number of packets per darknet port ``dst_port`` \n",
    "**NOTE**: for each individual source IP address, get all the different ports and their corresponding number of rows, that is the number of packets for each port sent from the source IP. Finally, calculate the average as requested. \n",
    "- Average number of packets per darknet IP ``dst_ip``\n",
    "- Average packet length\n",
    "- Average packet ttl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bee74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_ip_df = df.groupby(\"src_ip\").count()\n",
    "print(grouped_by_ip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac7179-e9ce-42b8-9f7b-fa1c198c2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "\n",
    "# Number of packets\n",
    "\n",
    "# Avg number packets per dst_port\n",
    "\n",
    "# Avg number packets per dst_ip\n",
    "\n",
    "# Avg pck_len and ttl\n",
    "\n",
    "# Merge all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849097ac-055f-4757-bb34-5283615834bc",
   "metadata": {},
   "source": [
    "Now you can load the Ground Truth (GT) dataset in ``ground_truth.csv``. Merge the GT labels with the obtained dataset with features per source IP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541394f0-2090-43b0-a66a-41553b0eea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce996a20-eb73-4be6-8daf-9d4b72c26168",
   "metadata": {},
   "source": [
    "Analyze the obtained features. For example, plot the ECDF of the IP senders with respect to the number of sent packets. Is the number of sent packets constant among the different source IPs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603e3e7-0e78-4bc4-872a-f6f4b0d56fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ECDF of number of packets \n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e156c-1c93-4123-b1f4-efec72c774e2",
   "metadata": {},
   "source": [
    "Finally, before proceeding with the clustering algorithm, standardize the dataset.\n",
    "\n",
    "**Note**: You are not using the labels (from the GT) as a feature, but keep track of them because you are going to use them later.\n",
    "\n",
    "**Note**: Since clustering is an unsupervised ML technique, it is not mandatory to split the dataset in training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33ca20-ca3b-4164-aa0c-47c22caac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817e2c9-d290-4f80-b343-fac86876d705",
   "metadata": {},
   "source": [
    "### 3.2 k-Means\n",
    "Let’s start from the first clustering algorithm: K-Means\n",
    "\n",
    "- Fit the k-Means algorithm with ``n_clusters=3`` (as the number of labels).\n",
    "- What is the size of the 3 clusters?\n",
    "- Compute the clustering error, the silhouette score (unsupervised metric), the Rand Index (RI) and Adjusted Rand Index (ARI) with respect to the GT (supervised metrics).<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602cb44-d258-4676-bd03-16bc14a736e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means with n_clusters = 3\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca5068-7a11-496b-b08c-2af87c4e981a",
   "metadata": {},
   "source": [
    "Now analyze how the performance changes by changing k. \n",
    "\n",
    "    1. Try k in the [2, 15] range. For each iteration, compute the different scores.\n",
    "    2. Plot the silhouette score for the different values of n_cluster. What is the best value of k, leading to the highest Silhouette.\n",
    "    3. Has the performance improved on the other metrics? Plot the other metrics for the different values of n_cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e28fb-a4ab-40ab-9327-7ec8efa07eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087e93c-e060-417e-a1e6-8c1bef2cdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6055b-cac7-4a7a-bf84-5c53781befd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c21f4-ac36-44cf-b596-dec52883b26b",
   "metadata": {},
   "source": [
    "To better understand the detected cluster, we can visualize the clusters.\n",
    "\n",
    "    - Reduce the feature dimensionality with a 2-component PCA. Fit the PCA on the full dataset and get the 2 Principal Components.\n",
    "    - Get the scatter plots of the samples on the 2-components. Repeat it three times by colouring the clusters:\n",
    "        1. K-Means clusters with 3 clusters\n",
    "        2. Best K-Means clusters from the heuristic\n",
    "        3. Original labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c692673-5f9c-4e89-ad2a-8700d30eb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D PCA\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8c867-4cc4-4edd-b680-c95e07e44e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTS\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b8567b-9e49-430e-94d7-a424e7d48f00",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 GMM\n",
    "\n",
    "Repeat the steps done in 3.2 for Gaussian Mixture models. Here the differences are:\n",
    "\n",
    "- Since here you have also the degree of belonging, report the effective size instead of the size of each cluster.\n",
    "- Consider the total log-likelihood score instead of the k-Means clustering error (sum of the squared error, aka inertia).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b30f3-bc78-4912-ae86-a1a52561cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM with 3 components\n",
    "\n",
    "# Your answer here\n",
    "\n",
    "# report effective size\n",
    "\n",
    "# Your answer here\n",
    "\n",
    "# report usupervised and supervised metric\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff66d7-371a-4ad3-9892-24fce3eaa932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now analyze how the performance changes by changing k. \n",
    "\n",
    "#1. Try k in the [2, 15] range. For each iteration, compute the different scores.\n",
    "\n",
    "# Your answer here\n",
    "\n",
    "#2. Plot the silhouette score for the different values of n_cluster. What is the best value of k, leading to the highest Silhouette.\n",
    "\n",
    "# Your answer here\n",
    "\n",
    "#3. Has the performance improved also on the other metrics? Plot the other metrics for the different values of n_cluster.\n",
    "\n",
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747b798-bcbb-4566-8d39-e9f626bae40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse 2D PCA\n",
    "# plots\n",
    "\n",
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c3424-8509-4df0-94a1-4bdc51d2f717",
   "metadata": {},
   "source": [
    "### 3.4 DBSCAN\n",
    "\n",
    "Repeat the steps done in 3.1 for DBSCAN. \n",
    "\n",
    "Being DBSCAN a density-based algorithm, you do not have to provide the number of clusters. Therefore, you cannot evaluate the performance by changing k. Instead, you have to perform a Grid Search on the ``epsilon`` and ``min_samples``. It consists of iterating over different values (combination) for two or more parameters, choosing the one leading to the best quality metrics.\n",
    "\n",
    "- Vary ``epsilon`` from 0.01 to 2 with a step 0.1 and ``min_samples`` from 0 and 20 with a step 1. For each pair of values, fit an individual DBSCAN, and get the labels as well as compute the silhouette. Afterwards, report the results with a heatmap and select the values leading to the highest silhouette. \n",
    "\n",
    "Finally, compare the results among the three algorithms and highlight their pros and cons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490fbec-da02-4298-a0df-c71c806220bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "\n",
    "# Your answer here\n",
    "\n",
    "# report number and size of each cluster\n",
    "\n",
    "# Your answer here\n",
    "\n",
    "# report usupervised and supervised metric\n",
    "\n",
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329505b-03ab-46f4-8088-968f24d5ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now analyze the performance varying epsilon and min_samples.\n",
    "\n",
    "# Your answer here\n",
    "\n",
    "# Plot the silhouette score for the different values with a heatmap\n",
    "\n",
    "# Your answer here\n",
    "\n",
    "# Report the results of the best combination, also on other metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a9970-d4ff-42f5-a9e8-b1d34b292054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse 2D PCA\n",
    "# plots\n",
    "\n",
    "# Your answer here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc60d50-e3fc-4b7d-9603-c7702f01de68",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Machine Learning for Networking <b><center>\n",
    "<center><b><font size=6>Lab 10 <b><center>\n",
    "<center><b><font size=6>   Neural networks <b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478461e-f007-4342-a199-e3562f6ca992",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LAB objective: define and train neural network models using Pytorch\n",
    "\n",
    "**Pytorch** is one of the most popular Python library for deep learning tasks and it is highly configurable. In this lab, you will learn how to define a feedforward neural network for supervised problems.\n",
    "\n",
    "Useful link: <a href=\"https://pytorch.org/docs/stable/index.html\">documentation</a>, <a href=\"https://pytorch.org/tutorials/beginner/basics/intro.html\">basics</a>, <a href=\"https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\">examples</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57514c-8374-456d-bb54-d8b6d9e99a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install pytorch. Then restart kernel. \n",
    "# WARNING: it takes around 5 minutes\n",
    "\n",
    "!python -m pip install torch\n",
    "!python -m pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbbbbbe-293d-4070-9447-5e9fcdfb0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed python libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ffcb2f-69a6-4afa-b1e5-3531d603312e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Tutorial - Neural networks for classifying hand-written digits\n",
    "We will use the MNIST dataset, which is a well-known image dataset for studying neural networks.\n",
    "It contains 70000 images (28x28 greyscale) of hand-written digit numbers from 0 to 9, and you need to define and train a neural network hypothesis to classify the image to the corresponding number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf04635-6da2-4c0a-83dc-a6aed91756c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and split into train, validation and test\n",
    "# the snippet here is customized for MNIST dataset, you don't have to know how it works exactly\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # mean and stardard deviation, already computed for MNIST dataset\n",
    "])\n",
    "\n",
    "dataset = MNIST('data/train', train=True, download=True, transform=transform)\n",
    "dataset_test = MNIST('data/test', train=False, download=True, transform=transform)\n",
    "\n",
    "# Further split intro training and validation set\n",
    "dataset_train, dataset_val = torch.utils.data.random_split(dataset, [int(len(dataset)*5/6), int(len(dataset)/6)])\n",
    "\n",
    "len(dataset_train), len(dataset_val), len(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dae6a29-7924-4f92-89b7-464a924a0955",
   "metadata": {},
   "source": [
    " We can visualize a random sample of the dataset. Run the cell multiple times to see different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02802a15-09cf-4da4-b053-702ec6993b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "n = random.randint(0, len(dataset_train)) #a random index\n",
    "image = dataset_train[n][0] # this is a 1x28x28 image. We represent it as grayscale (it is not RGB image)\n",
    "label = dataset_train[n][1]\n",
    "plt.imshow(image[0, :], cmap='Greys') \n",
    "plt.title(f'The number is {label}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16cc772-2b90-412f-aaf6-b96ca2bbbbe5",
   "metadata": {},
   "source": [
    " We define a function to compute accuracy and plot confusion matrix. We will use them multiple times later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e73e32-8c73-4b1a-835a-0af1676f6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function that calculate the accuracy\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # .item() to convert a tensor into a primitive type (float or int).\n",
    "    acc = (correct / len(y_pred))\n",
    "    return acc\n",
    "\n",
    "# function to compute and plot the confusion matrix\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    df = pd.DataFrame([x for x in zip([x.item() for x in y_true], [x.item() for x in y_pred])], columns=['y_true', 'y_pred'])\n",
    "    df[['samples']] = 1\n",
    "    confusion = pd.pivot_table(df, index='y_true', columns='y_pred', values='samples', aggfunc=sum)\n",
    "    plt.figure()\n",
    "    sns.heatmap(confusion, cmap='Blues', annot=True, cbar_kws={'label':'Occurrences'}, fmt='g')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('True')    \n",
    "    plt.title('Confusion matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f70f8b-819e-440a-a731-c81c52376798",
   "metadata": {},
   "source": [
    " Transform data into tensors and generate features and labels for train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e54e0-ee7c-46fc-bbf5-83ba13bbbd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform data into tensor X and tensor y\n",
    "def get_data_label(dataset):\n",
    "    # Initialize lists to store training data and labels\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate through the dataset to extract data and labels\n",
    "    for datum, label in dataset:\n",
    "        data.append(datum)\n",
    "        labels.append(torch.tensor([label]))\n",
    "    \n",
    "    # Concatenate the data and labels lists into tensors\n",
    "    data = torch.cat(data, dim=0)\n",
    "    labels = torch.cat(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1580d03-c512-4a09-85cb-6d68ec9c0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract all data and labels from the datasets. It may take a while.\n",
    "X_train, y_train = get_data_label(dataset_train)\n",
    "print(f\"Training set length: {X_train.shape[0]}\")\n",
    "\n",
    "X_val, y_val = get_data_label(dataset_val)\n",
    "print(f\"Validation set length: {X_val.shape[0]}\")\n",
    "\n",
    "X_test, y_test = get_data_label(dataset_test)\n",
    "print(f\"Test set length: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7292a-618a-4bde-9bc2-8a244a75aac5",
   "metadata": {},
   "source": [
    " The model architecture we are defining is a feed-forward neural network. It takes the input, feeds it through two hidden layers one after the other, and then finally gives the output. The input layer has many neurons as the pixels of the images, i.e., 28x28=784 neurons. The first hidden layer has 784 neurons and uses a tanh activation function. The second layer has 392 neurons and uses a tanh activation function. The output layer has 10 neurons, with softmax activation function.\n",
    "\n",
    "![](basic_NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe37aa02-9916-4927-86fb-7621629b511a",
   "metadata": {},
   "source": [
    "#### A typical training procedure for a neural network is as follows:\n",
    "1. Define the neural network, the loss function and the optimizer\n",
    "2. Iterate over a dataset of training inputs (training epochs)\n",
    "3. Forward step: process input through the network\n",
    "4. Compute the loss (empirical risk) \n",
    "5. Backward step (backpropagation): Compute and propagate gradients back into the network’s parameters\n",
    "6. Update the weights of the network, typically using a variation of gradient descent step: $weight = weight - learning \\ rate \\times gradient$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d657f-8450-4624-a4a6-34ed98aecd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network in the picture\n",
    "\n",
    "class Model_classification(nn.Module): # a class that inherits from the module of pytorch neural network \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features, # number of input features\n",
    "        out_features, # number of output features\n",
    "        hidden_1, # number of neurons in the 1st hidden layer\n",
    "        hidden_2 # number of neurons in the 2nd hidden layer\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=in_features, out_features=hidden_1) # 1st linear layer\n",
    "        self.layer_2 = nn.Linear(in_features=hidden_1, out_features=hidden_2) # 2nd linear layer\n",
    "        self.layer_output = nn.Linear(in_features=hidden_2, out_features=out_features) # output layer\n",
    "        self.activation_1 = nn.Tanh() # activation function\n",
    "        self.activation_2 = nn.Softmax() # activation function\n",
    "\n",
    "    # define feedforward process\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1) #to go from 28x28 image tensor to 784 input tensor\n",
    "        out = self.activation_1(self.layer_1(x)) #output of 1st hidden layer\n",
    "        out = self.activation_1(self.layer_2(out)) #output of 2nd hidden layer\n",
    "        out = self.activation_2(self.layer_output(out)) #output of the nn (a logit!)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3437e49-1fcb-4ac0-bd1c-cf20537ea89d",
   "metadata": {},
   "source": [
    "- For loss function, we are going to use the cross entropy loss\n",
    "- For optimizer, the ADAM optimizer (a variant of classic gradient descent  https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc5edd-0345-475b-b8a9-dfe9380bf0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed \n",
    "torch.manual_seed(8)\n",
    "\n",
    "# initialize the model with the correspoding hyper-parameters\n",
    "dim_features = len(X_train[0].flatten()) #784 = 28 x 28\n",
    "model = Model_classification(\n",
    "    in_features = dim_features, \n",
    "    out_features = 10, \n",
    "    hidden_1 = dim_features, \n",
    "    hidden_2 = int(dim_features / 2)\n",
    ")\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# define the optimizer (pass the parameters (model) that you want to optimize, and the learning rate)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7003a-17bd-4126-aac4-633d74865715",
   "metadata": {},
   "source": [
    "Let's train the network for 10 epochs and keep track of the loss and accuracy on the training and validation after each epoch.\n",
    "\n",
    "**Note** : if you run the following cell multiple times, the weights will not be re-initialized. Hence, you will continue the previous training each time. If you want to re-initialize the weights, re-define the model (i.e., run the previous cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fa316-6f96-420b-89c4-3e61122aa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "\n",
    "loss_train_all = []\n",
    "loss_val_all = []\n",
    "acc_train_all = []\n",
    "acc_val_all = []\n",
    "epochs = 10\n",
    "\n",
    "# Iterate over a dataset of inputs (training epochs)\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # model training phase\n",
    "    model.train() #initialize training phase \n",
    "    y_prob = model(X_train).squeeze() #  Process input through the network. Get the output probability of predictions. We are using all the data and not in batches\n",
    "    loss = loss_fn(y_prob, y_train) # calculate the empirical risk (average of the losses) \n",
    "    optimizer.zero_grad() # reset the gradients of model parameters\n",
    "    loss.backward() # backpropagate the empirical risk (prediction loss)\n",
    "    optimizer.step() # adjust the parameters by the gradients \n",
    "\n",
    "    # model evaluation phase\n",
    "    model.eval() #initialize evaluation phase \n",
    "    \n",
    "    # get metrics for the training set\n",
    "    y_prob = model(X_train).squeeze() #  Process input through the network. Get the output probability of predictions. We are using all the data and not in batches\n",
    "    y_pred = torch.argmax(y_prob, dim=1) # get the label --> multiclass, select the max of the softmax output\n",
    "    acc_train = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # get metrics for the validation set\n",
    "    y_prob = model(X_val).squeeze() \n",
    "    loss_val = loss_fn(y_prob, y_val) \n",
    "    y_pred = torch.argmax(y_prob, dim=1)\n",
    "    acc_val = accuracy_fn(y_true=y_val, y_pred=y_pred)\n",
    "\n",
    "    # collect results\n",
    "    loss_train_all.append(loss.item())\n",
    "    loss_val_all.append(loss_val.item())\n",
    "    acc_train_all.append(acc_train)\n",
    "    acc_val_all.append(acc_val)\n",
    "    \n",
    "    print(f'Epoch: {epoch} | Train Loss: {loss:.5f}, Val Loss: {loss_val:.5f}, Train Acc: {acc_train:.2f}, Val Acc: {acc_val:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d306b4d-49ac-4a55-9b55-1d94053201b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the evolution of losses and accuracies\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,3))\n",
    "\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.plot(loss_train_all, label='Train', color='blue')\n",
    "ax1.plot(loss_val_all, label='Val', color='red')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.plot(acc_train_all, label='Train', color='blue')\n",
    "ax2.plot(acc_val_all, label='Val', color='red')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0945265-6b84-4711-b497-86d15b546a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize (final) confusion matrix on validation set\n",
    "y_prob = model(X_val).squeeze() \n",
    "y_pred = torch.argmax(y_prob, dim=1)\n",
    "acc_val = accuracy_fn(y_true=y_val, y_pred=y_pred)\n",
    "print('The final validation accuracy is:', acc_val)\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747b000-1b99-480d-bad8-c25fa84c1dfe",
   "metadata": {},
   "source": [
    "#### Now you can try to repeat the process by changing some of the hyper-parameters of the neural network:\n",
    "- number of layers\n",
    "- number of neurons per layer\n",
    "- activation functions\n",
    "\n",
    "#### And some of the hyper-parameters of the training procedure:\n",
    "- number of epochs\n",
    "- learning rate\n",
    "- version of the optimizer            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a689240-d91c-4552-9064-b4c78bc9dd4c",
   "metadata": {},
   "source": [
    "Finally, apply the chosen hypothesis on test data and measure the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b4e39-647e-4916-9981-d7d83f78baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the model to test dataset\n",
    "\n",
    "model.eval()\n",
    "y_prob = model(X_test).squeeze()\n",
    "y_pred = torch.argmax(y_prob, dim=1)\n",
    "acc = accuracy_fn(y_true=y_test, y_pred=y_pred)\n",
    "print('The test accuracy is:', acc)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556fa46-b2c2-4c46-aca3-a7fc07762067",
   "metadata": {},
   "source": [
    "Let's see some examples of samples with their prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ea93b-4260-4b13-bc76-4705e2bf149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "model.eval()\n",
    "n = random.randint(0, len(dataset_test))\n",
    "image = X_train[:1] # the model always expect a batch of data to pass\n",
    "label = y_train[0]\n",
    "y_prob = model(image).squeeze()\n",
    "y_pred = torch.argmax(y_prob)\n",
    "plt.imshow(dataset_test[n][0][0, :], cmap='Greys') \n",
    "plt.title(f'The prediction is {y_pred}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f811e-c76e-4861-808f-86a32c1a33af",
   "metadata": {},
   "source": [
    "Now let's see an example of misclassified sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cedc1-3685-420f-b9ff-d66b85365c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(dataset_test)):\n",
    "    model.eval()\n",
    "    y_prob = model(X_train[n:n+1]).squeeze()\n",
    "    y_pred = torch.argmax(y_prob)\n",
    "    true = y_train[n]\n",
    "    if y_pred != true:\n",
    "        plt.figure()\n",
    "        plt.imshow(dataset_test[n][0][0, :], cmap='Greys')\n",
    "        plt.title(f'The Truth is {true}\\nThe prediction is {y_pred}')\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a40bf8-f142-4f8d-93ac-d5b4f999f061",
   "metadata": {},
   "source": [
    "### Deal with numpy data\n",
    "\n",
    "For MNIST, pytorch handled the data transformation automatically, which is often not the case in reality. **Importantly, pytorch only works with torch tensor.** Therefore, for pandas or numpy data, the first thing you need to do after data preprocessing and before feeding the data to the torch model is to convert your data into tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2b692-f939-48d9-89b2-83c93373c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 5x3 matrix (numpy multidimensional ndarray)\n",
    "x = np.random.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea250c3c-4304-43a6-800d-41e6c29a2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the numpy multidimensional ndarray into a pytorch tensor\n",
    "x_t = torch.tensor(x, dtype=torch.float, requires_grad=False) # Is True if gradients need to be computed for this Tensor, for input data is normally False\n",
    "x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9289e2-9a90-45b6-9106-db184550069a",
   "metadata": {},
   "source": [
    "### Some remarks\n",
    "1. For almost every function that you need to implement, like data loading, accuracy computation, batches and early stopping, you can find an implementation in pytorch or other libraries. In this lab, you can try to explore them on your own or implement the functions from scratch. \n",
    "2. It is easy to make errors regarding the dimension of tensors. Hint: use ``.shape`` to check out the shape of a tensor, and then check again the shape after performing certain operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666ee23-0ae9-4257-aa88-444b892ee7c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Exercise - Neural networks for classifying service of TCP flow logs\n",
    "\n",
    "The dataset 'log_tcp_complete_classes.txt' contains traffic information generated by an open-source passive network monitoring tool called **tstat**. It automates the collection of packet statistics of traffic aggregates, using real-time monitoring features. Being a passive tool, the typical usage scenario is live monitoring of Internet links, in which all transmitted packets are observed.\n",
    "\n",
    "In case of TCP, Tstat identifies a new flow start when it observes a TCP three-way handshake. Similarly, it identifies a TCP flow end either when it sees the TCP connection teardown, or when it doesn’t observe packets for some time (idle time). \n",
    "A flow is defined by a unique link between the sender and receiver, e.g., a tuple of <em>(IP_Protocol_Type, IP_Source_Address, Source_Port, IP_Destination_Address, Destination_Port)</em>. For a specific flow, tstat calculates a number of statistics of all the packets transmitted over this flow, and then generates a log for such flow with multiple attributes (statistics). \n",
    "\n",
    "A log file is arranged as a simple table where each column is associated to specific information and each row reports the flow during a connection. The log information is a summary of the flow properties. For instance, in the TCP log we can find columns like the starting time of a TCP connection, its duration, the number of sent and received packets, the observed Round Trip Time.\n",
    "\n",
    "In this exercise, you need to predict the service the client connected to in each flow, given the features of the TCP connection. The ``class:207`` is our label, and contains 10 different services, such as ``amazon`` and ``google``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5ed32-505c-4fc8-84ee-042c009efa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"log_tcp_complete_classes.txt\", sep=\" \")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501683a-0045-4555-b11e-eaadbbf5eb41",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Dataset pre-processing\n",
    "\n",
    "1. In the dataset, there are some categorical features (columns): ``#31#c_ip:1, c_port:2, s_ip:15, s_port:16, con_t:42, p2p_t:43, http_t:44, p2p_st:59, http_res:113, c_tls_SNI:116, s_tls_SCN:117, c_npnalpn:118, s_npnalpn:119, fqdn:127, dns_rslv:128``, in which ``#31#c_ip:1, c_port:2, s_ip:15, s_port:16, c_tls_SNI:116, s_tls_SCN:117, fqdn:127`` are required to be dropped.\n",
    "2. The column ``class:207`` is our label and it is also a categorical one, and you need to convert it to numerical labels to represent different connections. You can refer to:\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(...)\n",
    "```\n",
    "3. Perform a stratified segmentation to split the dataset into training, validation, and test sets, with a portion of 1%, 9%, and 90%. We are choosing a very small portion for training set to make the training faster for this exercise. In general, it is not a good idea to use just 1% of samples for training.\n",
    "4. For the rest of categorical features (columns) that remain in the dataset, you need to use one-hot encoding to convert the categories, by using ``OneHotEncoder`` to replace the original categorical features to one-hot features. Specifically, the function will compute the number of distinct categories, and then generate as many 0/1 variables (columns) as there are different categories. For example, if a feature has 3 distinct values, you will end up with 3 columns with 0 and 1 in the form of (0,0,1), (0,1,0), and (1,0,0), in which each tuple represents one of the original categories. Specifically:\n",
    "    - You can record the numerical index of the remaining categorical columns in the original dataframe\n",
    "    - After performing data segmentation, you end up with numpy arrays. Since you know which columns represent categorical features according to the previous step, you can first generate one-hot features of training set and then delete original categorical features, combining remaining numerical features and one-hot features: \n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore') # specify that you ignore possible unseen categories\n",
    "enc.fit(X_train[:, idx_columns_categorical_needed])\n",
    "features_enc_train = enc.transform(X_train[:, idx_columns_categorical_needed]).toarray()\n",
    "... = np.delete(...)\n",
    "... = np.concatenate([..., ...])\n",
    "```\n",
    "    - The ```OneHotEncoder``` needs to be fitted on training set, and then transform validation and test set. For each dataset, you need to remove the original categorical features.\n",
    "5. Standardize the dataset as always, fitting the scalar on training set and then trasforming the entire dataset.\n",
    "6. Convert your data to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff381b7e-2fe9-4ab0-b3dd-34b714bd08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "columns_categorical = [\n",
    "    '#31#c_ip:1', 'c_port:2', 's_ip:15', 's_port:16', 'con_t:42',\n",
    "    'p2p_t:43', 'http_t:44', 'p2p_st:59', 'http_res:113', 'c_tls_SNI:116',\n",
    "    's_tls_SCN:117', 'c_npnalpn:118', 's_npnalpn:119', 'fqdn:127', 'dns_rslv:128'\n",
    "]\n",
    "\n",
    "columns_drop = [\"#31#c_ip:1\", \"c_port:2\", \"s_ip:15\", \"s_port:16\", \"c_tls_SNI:116\", \"s_tls_SCN:117\", \"fqdn:127\"]\n",
    "\n",
    "df.drop(columns=columns_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341354d-8250-45dd-9ac3-c2c8b6d800f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2\n",
    "### Your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec62fa-ded3-42d6-8ee8-5034b58028ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3\n",
    "### Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3a08c-44e3-412e-a996-b19a43b07dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4\n",
    "### Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdba8a8-dd9c-4e95-a001-2ea5cef412c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 5\n",
    "### Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb98d60-6b39-4e01-813a-f010e520d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 6\n",
    "### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b076767-d4b6-4878-bd13-2a6e816014eb",
   "metadata": {},
   "source": [
    "### 2.2 Define the neural network model, the loss function and the optimizer\n",
    "\n",
    "Develop your model with an architecture that you think is proper, e.g., two hidden layers with number of neurons equivalent to the number of features. Specifically:\n",
    "- Build you model class and then initialize your model\n",
    "- Define activation function for each layer (e.g., tanh for hidden layers and softmax for output layer) \n",
    "- Define the optimizer with learning rate (e.g., Adam with learning rate 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eac605-719a-4781-9f73-598cfa3aab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here\n",
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d74d6-83ff-4960-95fb-8365a66a1f8e",
   "metadata": {},
   "source": [
    "### 2.3 Build your basic training loop\n",
    "Build you training loop that is similar to the tutorial, specifically:\n",
    "- Set the number of epochs to be 50 (this may take a while during the training)\n",
    "- Build you training loop including training phase and validation phase\n",
    "- At each epoch, record the losses and accuracies for training and validation sets. Print the current losses and accuracies for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ee14d-e34a-4a13-b40c-3b3a762cd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here\n",
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44216257-402c-4b07-b82c-d36605e4f7c0",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate your model\n",
    "1. Plot the evolution of losses and accuracies for training and validation set.\n",
    "2. Print the accuracy and plot confusion matrix of the final hypothesis on validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f065b73-3a11-4756-9c39-40f3b71d5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here\n",
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13180208-d060-4095-984a-43b58aabf675",
   "metadata": {},
   "source": [
    "### 2.5 Advanced training\n",
    "1. Change the previous training loop, implementing/using two important mechanisms:\n",
    "    - Data batch: At each epoch, split the training dataset into batches (e.g., 64 samples) and train the model on each batch sequentially. At each trial of training (for each batch), calculate the loss, and the final loss of the entire training set is calculated (at the end of this epoch), averaging the losses of all batches. You can realize batch training on your own or rely on functions provided by pytorch, like ``DataLoader`` (<a href=\"https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\">documentation</a>).\n",
    "    - Early stopping: Set the training epochs to infinite, and implement your own early stopping, terminating the training loop when the accuracy on validation is not improving anymore. Again, you can build your own early-stopping mechanism, or learn the function provided by pytorch (<a href=\"https://pytorch.org/ignite/generated/ignite.handlers.early_stopping.EarlyStopping.html\">documentation</a>).\n",
    "\n",
    "Remeber to re-initialize the model and re-define the optimizer.\n",
    "\n",
    "2. Evaluate the model:\n",
    "    - Plot the evolution of losses and accuracies for training and validation set, and indicate the epoch where the model stopped.\n",
    "    - Print the accuracy and plot confusion matrix of the final hypothesis on validation set. \n",
    "    - Answer the following questions:\n",
    "        - At how many epochs the early stopping stops the training? Is it very different from the previous one? If yes, why?\n",
    "        - At the same number of epochs, Which training procedure generates better performance (with or without data batch) ?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d116c892-dc74-45c8-9e01-8bfda301c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here\n",
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4012268-ec01-413d-8e6c-ec1c989da246",
   "metadata": {},
   "source": [
    "### 2.6 Evaluate final hypothesis on test set\n",
    " \n",
    "Use the best hypotesis on test set: derive the final performance by outputing accuracy and confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249de9b5-bd6b-4bcf-a159-99464936bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here\n",
    "### ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
